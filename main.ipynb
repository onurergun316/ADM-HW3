{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce21419-204f-4f21-8495-f3ba55fd460d",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24cc26-0ed6-470e-8f0f-84d5d8b85f06",
   "metadata": {},
   "source": [
    "### 1.1 Get the List of Animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8e1db3-5871-44b4-b156-5402a9c12434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import urllib.request \n",
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be5120-a410-4177-b788-3b8eafc60af7",
   "metadata": {},
   "source": [
    "Downloading each anime's url to a single text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07efba06-f07b-4e2c-ae55-8d7cbb44d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [05:44<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "links_text = open(\"links.txt\", \"w\")\n",
    "for page in tqdm(range(0, 400)):\n",
    "    url = 'https://myanimelist.net/topanime.php?limit=' + str(page * 50)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for tag in soup.find_all('tr'):\n",
    "        links = tag.find_all('a')\n",
    "        for link in links:        \n",
    "            if type(link.get('id')) == str and len(link.contents[0]) > 1:\n",
    "                data = link.get('href')\n",
    "                links_text.write(data)\n",
    "                links_text.write(\"\\n\")\n",
    "links_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaac8c0-38db-4a7d-b552-52c0025c78d9",
   "metadata": {},
   "source": [
    "Reading how many lines in the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "994484d5-158e-4049-8701-45936a73b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 19124 lines in this file.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"links.txt\", \"r\")\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print('There are total {} lines in this file.'.format(line_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a524d2-565f-4e71-b3f4-a54f6762c173",
   "metadata": {},
   "source": [
    "### 1.2 Crawl the Animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fda82ae7-a073-42a6-a25f-e3a7266a5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'html_pages'\n",
    "parent_dir = \"/Users/onurergun/Desktop/ADM-HW3\"\n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8dfd87-ca7f-4d30-9d3d-c62d75af510c",
   "metadata": {},
   "source": [
    "Creating subfolders for each page of the list </br>\n",
    "After creating the subfolders, we add each html file to corresponding page number folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "212ee972-7b5b-44cd-93b1-d5e95b2057a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the directory for html pages to be downloaded\n",
    "directory = 'html_pages'\n",
    "file_read = open('links.txt', 'r')\n",
    "anime_urls_list = file_read.readlines()\n",
    "file_read.close()\n",
    "\n",
    "for i in range(1,401):\n",
    "    # reading each page from 1 to 400\n",
    "    html_page_name = 'page'+str(i)\n",
    "    os.makedirs(os.path.join(directory, html_page_name ))\n",
    "    directory_subfolder = directory+'/'+html_page_name+'/'\n",
    "    \n",
    "    # reading each anime on the list from 1 to 51\n",
    "    for j in range(1,51):\n",
    "        anime_num = 50*(i-1)+j\n",
    "        html_file_name = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "        temp_text = open(html_file_name, \"w\")\n",
    "        url = anime_urls_list[(anime_num-1)].encode('ascii','backslashreplace').decode('utf-8')\n",
    "        urllib.request.urlretrieve(url,html_file_name)\n",
    "        temp_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6eede5-7e17-4b54-be30-1aca615714f0",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b58bf8-e034-4326-b495-4aa73271f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "animeTitle = []\n",
    "animeType = []\n",
    "animeNumEpisode = []\n",
    "releaseDate = []\n",
    "endDate = []\n",
    "animeNumMembers = []\n",
    "animeScore = []\n",
    "animeUsers = []\n",
    "animeRank = []\n",
    "animePopularity = []\n",
    "animeDescription = []\n",
    "animeRelated = []\n",
    "animeCharacters = []\n",
    "animeVoices = []\n",
    "animeStaff = []\n",
    "directory = 'html_pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f522dff-cd24-4809-b767-529ffb1f784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(html_file_path):\n",
    "    \"\"\"\n",
    "    Function that extracts anime's informations.\n",
    "    Input: path (a string that is related to the position of each anime page in the folder tree)\n",
    "    Output: a list of lists with all the informations mentioned above\n",
    "    \"\"\"\n",
    "    # take article_i.html from the directory \n",
    "    soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"spaceit_pad\"})\n",
    "    try:\n",
    "        animeTitle.append(str(soup.find_all('strong')[0].contents[0]))\n",
    "    except:\n",
    "        animeTitle.append('')\n",
    "\n",
    "    for div in divs:\n",
    "        spans = div.find_all(\"span\")\n",
    "        for span in spans:\n",
    "            # TYPES\n",
    "            if span.contents[0] == 'Type:':\n",
    "                try:\n",
    "                    animeType.append(str(div.find_all('a')[0].contents[0]))\n",
    "                except:\n",
    "                    animeType.append('NA')\n",
    "            # NUMBER OF EPISODES\n",
    "            if span.contents[0] == 'Episodes:':\n",
    "                try: \n",
    "                    animeNumEpisode.append(int(div.contents[2]))\n",
    "                except:\n",
    "                    animeNumEpisode.append(0)\n",
    "            # DATES\n",
    "            if span.contents[0] == 'Aired:':\n",
    "                try:\n",
    "                    if len(div.contents[2]) > 21:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        end = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        endDate.append(end)\n",
    "                    else:\n",
    "                        release = pd.to_datetime(div.contents[2][1:16]).to_pydatetime().strftime('%m/%d/%Y')\n",
    "                        releaseDate.append(release)\n",
    "                        endDate.append('-')\n",
    "                except:\n",
    "                        releaseDate.append('')\n",
    "                        endDate.append('')\n",
    "\n",
    "    divs = soup.find_all(\"div\", {\"class\": \"stats-block po-r clearfix\"})\n",
    "    for div in divs:\n",
    "        \n",
    "        # MEMBERS\n",
    "        members = div.find_all(\"span\", {\"class\": \"numbers members\"})\n",
    "        animeNumMembers.append(int(members[0].contents[1].contents[0].replace(',', '')))\n",
    "        \n",
    "        \n",
    "        # SCORE\n",
    "        # center of the html page\n",
    "        rating=soup.find(name=\"div\",attrs={\"class\":\"fl-l score\"})\n",
    "        try:        \n",
    "            animeScore.append(float(rating.text.strip()))\n",
    "        except:\n",
    "            animeScore.append(None)\n",
    "\n",
    "     \n",
    "        # USERS\n",
    "        users = div.find_all(\"div\", {\"class\": \"fl-l score\"})\n",
    "        # here we we eliminate the word 'user '   \n",
    "        # that is why there is the [:-6] part\n",
    "        # we also replace the comma divisor\n",
    "        try:\n",
    "            animeUsers.append(int(users[0]['data-user'][:-6].replace(',', '')))\n",
    "        except:\n",
    "            animeUsers.append(0)\n",
    "\n",
    "\n",
    "        # RANK\n",
    "        rank = div.find_all(\"span\", {\"class\": \"numbers ranked\"})\n",
    "        try:\n",
    "            animeRank.append(int(rank[0].contents[1].contents[0][1:]))\n",
    "        except:\n",
    "            animeRank.append(None)\n",
    "\n",
    "        # POPULARITY\n",
    "        popularity = div.find_all(\"span\", {\"class\": \"numbers popularity\"})\n",
    "        animePopularity.append(int(popularity[0].contents[1].contents[0][1:]))\n",
    "    \n",
    "    # DESCRIPTION\n",
    "    # center of the html page\n",
    "    animeDescription.append(soup.find_all(\"p\", itemprop = \"description\")[0].text.strip().replace('\\n', '').replace('  ', ''))\n",
    "\n",
    "\n",
    "    # RELATED \n",
    "    related = soup.find_all(\"table\", {\"class\": \"anime_detail_related_anime\"})\n",
    "    if(len(related)!=0):\n",
    "        x = []\n",
    "        y = []\n",
    "        for tr in related:\n",
    "            td = tr.find_all(\"td\")\n",
    "            for i in range(0, len(td), 2):\n",
    "                x.append(td[i].contents[0])\n",
    "                t = td[i+1].find_all(\"a\")\n",
    "                if(len(t[0].contents)!=0):  \n",
    "                    y.append(t[0].contents[0])\n",
    "                else:\n",
    "                    y.append(' ')\n",
    "            animeRelated.append('\\n'.join([f'{x} {y}' for x, y in dict(zip(x, y)).items()]).split('\\n'))\n",
    "    else:\n",
    "        animeRelated.append(' ')\n",
    "    \n",
    "    # CHARACTERS\n",
    "    try:\n",
    "        characters = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        chars = characters[0].find_all(\"h3\", {\"class\": \"h3_characters_voice_actors\"})\n",
    "        x = []\n",
    "        for i in chars:\n",
    "            x.append(i.contents[0].contents[0])\n",
    "        animeCharacters.append(x)\n",
    "    except:\n",
    "        animeCharacters.append(\" \")\n",
    "    \n",
    "    \n",
    "   # VOICES\n",
    "    try:\n",
    "        voices = characters[0].find_all(\"td\", {\"class\": \"va-t ar pl4 pr4\"})\n",
    "        y = []\n",
    "        for i in voices:\n",
    "            y.append(i.contents[1].contents[0])\n",
    "        animeVoices.append(y)\n",
    "    except:\n",
    "        animeVoices.append(\" \")\n",
    "    \n",
    "    # STAFF\n",
    "    try:\n",
    "        staff = soup.find_all(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "        staff = staff[1].find_all(\"td\")\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(1, len(staff), 2):\n",
    "            x.append(staff[i].contents[1].contents[0])\n",
    "            y.append(staff[i].find_all(\"small\")[0].contents[0])\n",
    "        animeStaff.append([list(i) for i in list(zip(x,y))])\n",
    "    \n",
    "    except:\n",
    "        animeStaff.append(\" \")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9001ff2c-b7cc-4e70-9491-a06c17f71e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tsv_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ed06c8-5cc5-4153-b38c-e8e14cc1b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create tsv files \n",
    "def tsv_create(i):\n",
    "    tsv_columns = ['animeTitle','animeType','animeNumEpisode','releaseDate','endDate','animeNumMembers','animeScore',\n",
    "                  'animeUsers','animeRank','animePopularity','animeDescription','animeRelated','animeCharacters',\n",
    "                  'animeVoices','animeStaff']\n",
    "    # each tsv files are read and written with the respective column names\n",
    "    data = zip([animeTitle[i-1]],[animeType[i-1]],[animeNumEpisode[i-1]],[releaseDate[i-1]],[endDate[i-1]],[animeNumMembers[i-1]],[animeScore[i-1]],[animeUsers[i-1]],[animeRank[i-1]],[animePopularity[i-1]],[animeDescription[i-1]],[animeRelated[i-1]],[animeCharacters[i-1]],[animeVoices[i-1]],[animeStaff[i-1]])\n",
    "    # opening the relative specific tsv file\n",
    "    tsv_file_name = 'tsv_files/anime_'+str(i)+'.tsv'\n",
    "    with open(tsv_file_name, 'w', newline='') as f_output:\n",
    "        # creating the tsv \n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        tsv_output.writerow(tsv_columns)\n",
    "        for title,typ,numEp,relD,endD,numMem,score,user,rank,popularity,descr,relat,charac,voices,staff in data:\n",
    "                tsv_output.writerow([title,typ,numEp,relD,endD,numMem,score,user,rank,popularity,descr,relat,charac,voices,staff])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2e7d98-3a8a-4395-944a-20a6d598ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,384):\n",
    "    html_page_name = 'page'+str(i)\n",
    "    directory_subfolder = directory+'/'+html_page_name+'/'\n",
    "    if(i!=383):\n",
    "        # 383th page has less than 50 animes thats why\n",
    "        # on the else block we only iterate until 25\n",
    "        for j in range(1,51):\n",
    "            # getting the anime index number according to which page we are in, i is the page, and j is the which jth element\n",
    "            # of the page we are in \n",
    "            anime_num = 50*(i-1)+j\n",
    "            # from the number we get we create the html file in article_i form\n",
    "            html_file_path = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "            soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "            parse_function(html_file_path)\n",
    "            # calling the tsv create function to create tsv \n",
    "            tsv_create(anime_num)\n",
    "    else:\n",
    "        for j in range(1,25):\n",
    "            anime_num = 50*(i-1)+j\n",
    "            html_file_path = directory_subfolder+'article_'+str(anime_num)+'.html'\n",
    "            soup = BeautifulSoup(open(html_file_path), \"html.parser\")\n",
    "            parse_function(html_file_path)\n",
    "            tsv_create(anime_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79389ea5-29bc-4631-a0ed-f83754e0a8b3",
   "metadata": {},
   "source": [
    "# 2. Search Engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdbe4b-c45e-448a-90bb-793d66cb2c93",
   "metadata": {},
   "source": [
    "### Preprocessing of the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "db132e30-99e9-4336-9a3b-60a32a6fea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "474e1e88-3153-4bef-af16-06d97299da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the english stopwords list\n",
    "stop = stopwords.words('english')\n",
    "# stemming object from porter stemmer library\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f466b710-6907-4798-9fc7-3ab0a4f4c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    # function gets the sentence and then splits into a words\n",
    "    tokens = sentence.split()\n",
    "    # each word are considered as tokens and we stem them accordingly using porter stemmer\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    # after we join all the list to a str and return it\n",
    "    return ' '.join(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1e525f46-f097-414e-ac82-3f2f0afa5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatingg an empty list for animeTitles and animeDescriptions\n",
    "animeTitle_list = []\n",
    "animeDescription_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6029eaab-8a87-4b23-91b3-ce28379cee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a lists from tsv files\n",
    "for i in range(0,19124):\n",
    "    # 19124 is the number of animes we have\n",
    "    # we open them accordingly and save it to a list we've just created \n",
    "    anime_tsv = open('tsv_files/anime_'+str(i+1)+'.tsv', 'r',encoding=\"utf8\")\n",
    "    data=pd.read_table(anime_tsv)[['animeTitle','animeDescription']]\n",
    "    data['animeTitle'] = data['animeTitle'].astype(str)\n",
    "    data['animeDescription'] = data['animeDescription'].astype(str)\n",
    "    # appending the titles and animes to a list\n",
    "    animeTitle_list.append(str(data.animeTitle[0]))\n",
    "    animeDescription_list.append(str(data.animeDescription[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1b848d3e-11cd-4955-a3df-2b79e3b44560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concetenating lists to create a dataframe\n",
    "anime_df = pd.DataFrame(np.column_stack([animeTitle_list, animeDescription_list]), \n",
    "                               columns=['animeTitle', 'animeDescription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1610c5-d6f9-4711-9c50-8483bf773aab",
   "metadata": {},
   "source": [
    "Checking if there is a mismatch on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aac76e83-4f8b-4600-94bd-9c63ad0db1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-201-b8d327cbacaf>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  anime_df['animeDescription'] = anime_df['animeDescription'].str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords from the dataframe\n",
    "anime_df['animeDescription']  = anime_df['animeDescription'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "# removing punctuations from the dataframe\n",
    "anime_df['animeDescription'] = anime_df['animeDescription'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# stemming the dataframe \n",
    "anime_df['animeDescription'] = anime_df['animeDescription'].apply(stem_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38eedf2d-21d4-4e8b-bd88-c1ef3d44aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a words list inside synopsis of each anime\n",
    "words_list = ' '.join([i for i in anime_df['animeDescription']]).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a4ac3-69fb-4ede-b763-55c862ff8296",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6a7472b4-b9a1-4e08-8b03-5787f9fd4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the duplicate words from the words list\n",
    "words_dict = set(words_list)\n",
    "\n",
    "# Assign a unique integer id to each non duplicate words\n",
    "vocabulary = {}\n",
    "# by doing this, we are gonna create a vocabulary dictionary, with each row has unique id key, assigned\n",
    "# to a non duplicate word inside all of the lists\n",
    "i=1\n",
    "for word in words_dict:\n",
    "    # adding word to i num key\n",
    "    vocabulary.update({i:word})\n",
    "    # increasing the key number \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7ec6e-92c7-4f12-98c0-32425be43830",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create your index!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc060b1f-9554-4094-a315-d16c1e07c747",
   "metadata": {},
   "source": [
    "Creating the vocabulary json file that we store for each nun duplicate words with id number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f97eb546-38d6-4637-a611-d18189379cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vocabulary json file and assinging all the values inside\n",
    "# vocabulary dictionary to a json file\n",
    "# so that we dont have to create vocabulary dictionary everytime we run the project\n",
    "with open(\"vocabulary.json\", \"w\") as file:\n",
    "    json.dump(vocabulary, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0c5b2-b2c2-41bb-92a4-a177382e9f9b",
   "metadata": {},
   "source": [
    "Creating an inverted index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "efd6565c-d18d-47d6-bfa8-41f0f941c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty dictionary to create a inverted index json file\n",
    "inverted_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5264ecd5-be4a-44fb-8c78-82720e0daf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36793/36793 [1:17:26<00:00,  7.92it/s]   \n"
     ]
    }
   ],
   "source": [
    "# reading vocabulary json\n",
    "with open('vocabulary.json') as data_file:  \n",
    "    # storing the json data inside a data object\n",
    "    data = json.load(data_file)\n",
    "    # reading each key and value pairs inside vocabulary json file\n",
    "    for key, value in tqdm(data.items()):\n",
    "        # creating an empty list for inverted json file\n",
    "        inverted_list = []\n",
    "        for i in range(0,len(anime_df)):\n",
    "            # parsing the synopsis sentences into words\n",
    "            if(value in anime_df['animeDescription'][i].split()):\n",
    "                # if the value of vocabulary json is inside the anime synopsis\n",
    "                # add anime name that has the value to a list \n",
    "                anime_name = 'anime_'+str(i+1)\n",
    "                inverted_list.append(anime_name)\n",
    "                # update the dict with the key and inverted list we've just created\n",
    "                inverted_dict.update({key:inverted_list})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a489b6-fcba-4535-a5ab-d0635c6b5365",
   "metadata": {},
   "source": [
    "Saving the inverted index dictionary to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6cf27e19-0ff6-4634-80c8-2f6009ad481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an inverted.json file and assign inverted_dict\n",
    "with open(\"inverted.json\", \"w\") as file:\n",
    "    json.dump(inverted_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ced05-8ed6-4d48-9312-149c9edd00bc",
   "metadata": {},
   "source": [
    "#### 2.1.2 Execute the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e36e46f7-653a-4d1a-8fc1-87a309aa4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_query(query_list):\n",
    "    # empty list to assign values at the end which has same values with the query\n",
    "    anime_query_list = []\n",
    "    # iterate over the query list, as on every word\n",
    "    for word in query_list:\n",
    "        # open the vocab json file\n",
    "        with open('vocabulary.json') as data_file:\n",
    "            data = json.load(data_file)\n",
    "            for key, value in data.items():\n",
    "                # if the word inside query matches the value in the vocabulary json\n",
    "                if(word == value):\n",
    "                    # open the inverted json and match the key with it\n",
    "                    with open('inverted.json') as inverted_file:\n",
    "                        inverted_data = json.load(inverted_file)\n",
    "                        for inv_key, inv_value in inverted_data.items():\n",
    "                            # if the keys are matching, then we have the same word query\n",
    "                            if(key == inv_key):\n",
    "                                # appending the value to a list if has the specific query word\n",
    "                                anime_query_list.append(inv_value)\n",
    "    \n",
    "    # creating a list from all animes including duplicate ones\n",
    "    anime_list = []\n",
    "    for i in range(len(anime_query_list)):\n",
    "        for j in range(len(anime_query_list[i])):\n",
    "            anime_list.append(anime_query_list[i][j])\n",
    "    \n",
    "    # creating a set to find non duplicate anime files\n",
    "    anime_query_set_list = list(set(anime_list))\n",
    "    # creating an empty list to store the final anime list which has all the input queries \n",
    "    anime_final_list = []\n",
    "    \n",
    "    # counting the occurences of each anime with the length of the total query\n",
    "    # if its equal to total len, then each word in the query appears on the anime description\n",
    "    for anime in anime_query_set_list:\n",
    "        if(anime_list.count(anime) == len(anime_query_list)):\n",
    "            anime_final_list.append(anime)\n",
    "    # returning a final anime list which includes every word inside the query        \n",
    "    return anime_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60412a0b-4da6-4556-a1d8-f71993382c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_anime_df(anime_list):\n",
    "    # creating lists for animes\n",
    "    animeTitle_list = []\n",
    "    animeDescription_list = []\n",
    "    animeUrl_list = []\n",
    "    # to creat dataframe, we first create empty lists representing each column for the dataframe\n",
    "    \n",
    "    # assigning tsv values from the animes to lists we've just created \n",
    "    for anime in anime_list:\n",
    "        anime_tsv = open('tsv_files/'+anime+'.tsv', 'r',encoding=\"utf8\")\n",
    "        data=pd.read_table(anime_tsv)[['animeTitle','animeDescription']]\n",
    "        data['animeTitle'] = data['animeTitle'].astype(str)\n",
    "        data['animeDescription'] = data['animeDescription'].astype(str)\n",
    "        animeTitle_list.append(str(data.animeTitle[0]))\n",
    "        animeDescription_list.append(str(data.animeDescription[0]))\n",
    "\n",
    "    # reading text file url lines to a list\n",
    "    f=open('links.txt')\n",
    "    url_lines=f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # creating a for loop to iterate over each anime we have on the anime_list\n",
    "    for anime in anime_list:\n",
    "        # getting the int value from the anime name\n",
    "        anime_num=(int(anime.split(\"anime_\",1)[1]))\n",
    "        # finding the corresponding line from the links.txt and assigning it to a list\n",
    "        animeUrl_list.append(url_lines[(anime_num-1)])\n",
    "\n",
    "    # creating the dataframe from lists and returning it\n",
    "    return pd.DataFrame(np.column_stack([animeTitle_list, animeDescription_list, animeUrl_list]), \n",
    "                                   columns=['animeTitle', 'animeDescription', 'Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0659a0a-a0fe-4fb0-a842-fe3ac53c8a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search: is the saiyan, racing?\n"
     ]
    }
   ],
   "source": [
    "# getting an input from the user\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c63ad4f-bd6a-411f-aad9-4667295776ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming, removing stop words and punctuations from the input string\n",
    "query = ' '.join([word for word in query.split() if word not in stop])\n",
    "query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "query = stem_sentences(query)\n",
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# getting the list of animes which has the query\n",
    "anime_list = find_query(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ed7059-6b81-4c7f-ac87-ab9aa7db3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the anime dataframe from our query\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e5ecdc-3d8e-44b7-818a-50cffee69564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "0                           Dragon Ball Super: Broly   \n",
       "1                                    Dragon Ball Kai   \n",
       "2                                      Dragon Ball Z   \n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "0  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "1  Five years after the events of Dragon Ball, ma...   \n",
       "2  Five years after winning the World Martial Art...   \n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://myanimelist.net/anime/36946/Dragon_Bal...  \n",
       "1  https://myanimelist.net/anime/6033/Dragon_Ball...  \n",
       "2  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ab6ea-3511-43e7-9f9f-0a7f121ae2e9",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077083b7-1cb5-490f-bc55-0f7909d02662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ce697-9796-4afc-842b-a6eb468ab649",
   "metadata": {},
   "source": [
    "### 2.2.1 Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b96321-8241-46d6-8b9a-216ffcb6c0d7",
   "metadata": {},
   "source": [
    "finding the term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4ca32ae-5e37-4a93-a7bc-f9866f607ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf(word, anime_num):\n",
    "    # finding the number of specific word in a document\n",
    "    document = anime_df['animeDescription'][anime_num-1].split()\n",
    "    tf_counter = 0\n",
    "    for i in range(len(document)):\n",
    "        if(document[i] == word):\n",
    "            tf_counter+=1\n",
    "    # calculating the term frequency aka tf\n",
    "    tf = tf_counter / len(document)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a008c-3f49-48fe-83a0-4480d2fc0965",
   "metadata": {},
   "source": [
    "finding the document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7475681f-87ff-43ec-af72-600bb0d6040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_df(word):\n",
    "    # finding the number of occurences of the word in all the documents\n",
    "    df_counter = 0\n",
    "    for i in range(len(anime_df)):\n",
    "        document = anime_df['animeDescription'][i].split()\n",
    "        for i in range(len(document)):\n",
    "            if(document[i] == word):\n",
    "                df_counter+=1 # df counter is the document frequency aka df\n",
    "    return df_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab0f2ab-8273-4e4d-9de0-d8ff0b9cb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.json') as word_file:\n",
    "    data_vocab = json.load(word_file)\n",
    "    word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eef24a7d-4b2e-4483-bfb0-f8976bf18e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the last version of the tfidf dict\n",
    "tfidf_last_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6853d164-5b52-4930-8262-aefbe5924d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36793/36793 [25:31:13<00:00,  2.50s/it]       \n"
     ]
    }
   ],
   "source": [
    "with open('inverted.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    data_file.close()\n",
    "    \n",
    "with open('vocabulary.json') as word_file:\n",
    "    data_vocab = json.load(word_file)\n",
    "    word_file.close()\n",
    "    \n",
    "for i in tqdm(range(len(data))):\n",
    "    # reading each anime to a list that contains a specific word\n",
    "    word = data_vocab[str(i+1)] # getting the specific word from vocabulary json\n",
    "    anime_list = data[str(i+1)] # getting all animes related to a specific word from inverted.json\n",
    "    tfidf_list = []\n",
    "    for anime in anime_list:\n",
    "        tfidf_dict = {}\n",
    "        # getting the anime num from the tsv file name\n",
    "        anime_num=(int(anime.split(\"anime_\",1)[1]))\n",
    "        # finding the tf value\n",
    "        tf = find_tf(word, anime_num) \n",
    "        # finding the df value\n",
    "        df_counter = find_df(word)\n",
    "        # inverse document frequency aka idf is a log of total documents divided by df+1\n",
    "        # idf = log(N/(df + 1))\n",
    "        idf = math.log((len(anime_df) / df_counter+1), 10)\n",
    "        # calculating the tfidf number by multiplication\n",
    "        tfidf = tf * idf\n",
    "        # creating a dictionary for each anime - tfidf pairs\n",
    "        tfidf_dict.update({anime:tfidf})\n",
    "        tfidf_list.append(tfidf_dict)\n",
    "    tfidf_last_dict.update({i+1:tfidf_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7cf9d-6782-454d-af58-b1d09dfd4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the json file\n",
    "with open(\"tfidf.json\", \"w\") as file:\n",
    "    json.dump(tfidf_last_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a4d3a-c0d4-4e75-9b54-c9430a34de0b",
   "metadata": {},
   "source": [
    "### 2.2.2 Execute the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8e8534d0-88ef-4ebc-9ac1-910ba45f0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3a82b681-f906-4d77-915c-410987c53e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    # the two string vectors intersection and numerator calculation\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    # summing up the values from the vectors\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    # calculating the denominator for the cosine\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    \n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        # return the cosine\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    # getting a string value and turning it to a vector \n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b342bfd3-a5f4-4664-89f8-460ab00e5f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search: saiyan race\n"
     ]
    }
   ],
   "source": [
    "# getting an input from the user\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "873d81fb-b818-4f51-bf30-00d9d37b61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming, removing stop words and punctuations from the input string\n",
    "query = ' '.join([word for word in query.split() if word not in stop])\n",
    "query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "query = stem_sentences(query)\n",
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# creating regex pattern object\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "# from the anime_list we find the query\n",
    "anime_list = find_query(query_list)\n",
    "# creating a dataframe from the anime list we got\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "38ef6f69-6bc0-4d8f-b563-f32209a95ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the anime numbers from the returned dataframe from the query\n",
    "anime_num_list = []\n",
    "for i in range(len(anime_df)):\n",
    "    for j in range(len(query_anime_df)):\n",
    "        if(anime_df['animeTitle'][i] == query_anime_df['animeTitle'][j]):\n",
    "            anime_num_list.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb0a1c2f-6291-4fa6-a5bd-dafd5cb41f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the tfidf json file and saving it to a dictionary\n",
    "with open('tfidf.json') as word_file:\n",
    "    data_tfidf = json.load(word_file)\n",
    "    word_file.close()\n",
    "    \n",
    "# opening the vocabulary json file and saving it to a dictionary\n",
    "with open('vocabulary.json') as word_file:\n",
    "    data_vocab = json.load(word_file)\n",
    "    word_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "53fcbb76-b067-4035-b845-11d11ded8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_score(query, anime_num):\n",
    "    tfidf_score_list = []\n",
    "    query_list = query.split()\n",
    "    # splitting the sentence to get each word\n",
    "    for word in query_list:\n",
    "        # for every word we iterate\n",
    "        for key, value in data_vocab.items():\n",
    "            # trying to find the word inside vocabulary.json file\n",
    "            if(word == value):\n",
    "                # if we find it, we match that key with tfidf json key\n",
    "                for tfidf_key, tfidf_value in data_tfidf.items():\n",
    "                    if(key == tfidf_key):\n",
    "                        # when we find the key, we find the scores for each anime\n",
    "                        for i in range(len(data_tfidf[tfidf_key])):\n",
    "                            for t_key, t_value in data_tfidf[tfidf_key][i].items():\n",
    "                                # finding the matching anime names \n",
    "                                if(anime_num == (int(t_key.split(\"anime_\",1)[1]))):\n",
    "                                    # appending the tfidf score to a list\n",
    "                                    tfidf_score_list.append(t_value)\n",
    "    # getting the average score of tfidf score\n",
    "    tfidf_score = sum(tfidf_score_list) / len(tfidf_score_list) \n",
    "    return tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3ea7a11f-27e3-46c5-94e1-25aa10766b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarity(anime_num_list, query):\n",
    "    similarity_score_list = []\n",
    "    # iterate over the query dataframe we created\n",
    "    for i in range(len(query_anime_df)):\n",
    "        # get the specific anime's synopsis\n",
    "        anime_synopsis = anime_df['animeDescription'][anime_num_list[i]-1]\n",
    "        # get the vectors of the query and the anime's synopsis\n",
    "        vector1 = text_to_vector(query)\n",
    "        vector2 = text_to_vector(anime_synopsis)\n",
    "        # calculating the cosine similarity\n",
    "        cosine = get_cosine(vector1, vector2)\n",
    "        # getting the average tfidf score from each query in the sentence\n",
    "        tfidf_score = get_tfidf_score(query, anime_num_list[i])\n",
    "        # finding the similarity score\n",
    "        similarity_score = cosine * tfidf_score\n",
    "        similarity_score_list.append(similarity_score)\n",
    "    return similarity_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "05e189fb-edd5-437e-b735-eabc04fbe3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the similarity_score\n",
    "similarity_score_list = find_similarity(anime_num_list, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0c64d3c8-6cf6-4ac4-81cf-fc1a4c6577d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending similarity score \n",
    "query_anime_df['similarity'] = similarity_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b0275891-a7cb-4ed6-bed2-33726e0b0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the values based on similarity on descending order by heapsort and getting the top5 results\n",
    "query_anime_df = query_anime_df.sort_values('similarity',ascending= False,kind = 'heapsort').head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d3e14f8d-eb1d-4adb-883c-2d62d19b2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>0.018094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>0.006454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.001686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1                                    Dragon Ball Kai   \n",
       "0                           Dragon Ball Super: Broly   \n",
       "2                                      Dragon Ball Z   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1  Five years after the events of Dragon Ball, ma...   \n",
       "0  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "2  Five years after winning the World Martial Art...   \n",
       "\n",
       "                                                 Url  similarity  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...    0.018094  \n",
       "1  https://myanimelist.net/anime/6033/Dragon_Ball...    0.006454  \n",
       "0  https://myanimelist.net/anime/36946/Dragon_Bal...    0.002246  \n",
       "2  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n    0.001686  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4f8d7-2a5a-4cf6-8405-2cdc24082d68",
   "metadata": {},
   "source": [
    "# 3. Define a new score!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878f81a-cea5-4c23-9653-0b59d6d38c97",
   "metadata": {},
   "source": [
    "For the new score metric we define as the following:</br>\n",
    "new_score = (similarity * ((animeScore/100) + 1/animePopularity))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "16daa7bb-121a-42d4-8aaa-54324d151937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_popularity_and_score_list(anime_num_list):\n",
    "    # finding the popularity and vote score values from the tsv files and appending to a list\n",
    "    animeScore_list = []\n",
    "    animePopularity_list = []\n",
    "    # for each anime we iterate over the list\n",
    "    for anime in anime_num_list:\n",
    "        # opening the tsv file\n",
    "        anime_tsv = open('tsv_files/anime_'+str(anime)+'.tsv', 'r',encoding=\"utf8\")\n",
    "        # getting the data\n",
    "        data=pd.read_table(anime_tsv)[['animeScore','animePopularity']]\n",
    "        # assigning the anime score value and popularity\n",
    "        data['animeScore'] = data['animeScore'].astype(str)\n",
    "        data['animePopularity'] = data['animePopularity'].astype(str)\n",
    "        animeScore_list.append(str(data.animeScore[0]))\n",
    "        animePopularity_list.append(str(data.animePopularity[0]))\n",
    "    \n",
    "    return animeScore_list, animePopularity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "5f43772c-ac12-4737-8435-8fab076713be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(animeScore_list, animePopularity_list):\n",
    "    for i in range(len(animeScore_list)):\n",
    "        # finding the animeScore weight by dividing it by 100\n",
    "        animeScore_list[i] = float(animeScore_list[i])/100\n",
    "        # finding the popularity score weight by dividing to 1 \n",
    "        animePopularity_list[i] = 1/(float(animePopularity_list[i]))\n",
    "    return animeScore_list,animePopularity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b36e4ed2-27f3-4771-bf92-fb057df19ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score(similarity_score_list, cal_animeScore, calanimePopularity):\n",
    "    new_score_list = []\n",
    "    for i in range(len(similarity_score_list)):\n",
    "        new_score = (similarity_score_list[i] * (cal_animeScore[i] + calanimePopularity[i]))*100\n",
    "        new_score_list.append(new_score)\n",
    "    return new_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1cab879a-abbb-4d6c-80e3-a4f64917ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search: saiyan race\n"
     ]
    }
   ],
   "source": [
    "# getting an input from the user\n",
    "query = input('Enter your search:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "cca38103-52d7-4d8a-b857-ad840e75ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming, removing stop words and punctuations from the input string\n",
    "query = ' '.join([word for word in query.split() if word not in stop])\n",
    "query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "query = stem_sentences(query)\n",
    "# creating a list from the input query\n",
    "query_list = query.split()\n",
    "# creating regex pattern object\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "anime_list = find_query(query_list)\n",
    "query_anime_df = create_query_anime_df(anime_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e4ff071a-494d-4683-90dd-e7b2894aa61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the anime numbers from the returned dataframe from the query\n",
    "anime_num_list = []\n",
    "for i in range(len(anime_df)):\n",
    "    for j in range(len(query_anime_df)):\n",
    "        if(anime_df['animeTitle'][i] == query_anime_df['animeTitle'][j]):\n",
    "            anime_num_list.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "0a93063a-a36b-4cbc-b174-07a6050a3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the similarity score\n",
    "similarity_score_list = find_similarity(anime_num_list, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "32f3574f-ca23-4302-9d05-a50d8bd74681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting popularity and score list\n",
    "animeScore_list, animePopularity_list = find_popularity_and_score_list(anime_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "731448b5-e965-4f30-be66-fdf7e827aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the calculated anime scores and popularities\n",
    "cal_animeScore, calanimePopularity = calculate_scores(animeScore_list, animePopularity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e7058841-bb03-4c1a-94d4-81fc99afbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the new score list\n",
    "new_score_list = new_score(similarity_score_list, cal_animeScore, calanimePopularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e1b4e40d-af9c-4cef-bf3b-01be540a4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a new column in a dataframe\n",
    "query_anime_df['new_score'] = new_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "861f2662-89d7-4d26-89b2-33f45dada414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heapsorting them in descending order for the top5 values (if there is)\n",
    "query_anime_df = query_anime_df.sort_values('new_score',ascending= False,kind = 'heapsort').head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "7d1792a1-7467-4b91-b7cb-514d0f9ef646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "      <th>new_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dragon Ball Z Special 1: Tatta Hitori no Saish...</td>\n",
       "      <td>Bardock, Son Goku's father, is a low-ranking S...</td>\n",
       "      <td>https://myanimelist.net/anime/986/Dragon_Ball_...</td>\n",
       "      <td>0.137620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dragon Ball Kai</td>\n",
       "      <td>Five years after the events of Dragon Ball, ma...</td>\n",
       "      <td>https://myanimelist.net/anime/6033/Dragon_Ball...</td>\n",
       "      <td>0.053426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dragon Ball Super: Broly</td>\n",
       "      <td>Forty-one years ago on Planet Vegeta, home of ...</td>\n",
       "      <td>https://myanimelist.net/anime/36946/Dragon_Bal...</td>\n",
       "      <td>0.020647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dragon Ball Z</td>\n",
       "      <td>Five years after winning the World Martial Art...</td>\n",
       "      <td>https://myanimelist.net/anime/813/Dragon_Ball_Z\\n</td>\n",
       "      <td>0.013312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          animeTitle  \\\n",
       "3  Dragon Ball Z Special 1: Tatta Hitori no Saish...   \n",
       "1                                    Dragon Ball Kai   \n",
       "0                           Dragon Ball Super: Broly   \n",
       "2                                      Dragon Ball Z   \n",
       "\n",
       "                                    animeDescription  \\\n",
       "3  Bardock, Son Goku's father, is a low-ranking S...   \n",
       "1  Five years after the events of Dragon Ball, ma...   \n",
       "0  Forty-one years ago on Planet Vegeta, home of ...   \n",
       "2  Five years after winning the World Martial Art...   \n",
       "\n",
       "                                                 Url  new_score  \n",
       "3  https://myanimelist.net/anime/986/Dragon_Ball_...   0.137620  \n",
       "1  https://myanimelist.net/anime/6033/Dragon_Ball...   0.053426  \n",
       "0  https://myanimelist.net/anime/36946/Dragon_Bal...   0.020647  \n",
       "2  https://myanimelist.net/anime/813/Dragon_Ball_Z\\n   0.013312  "
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12745c-9cc6-43ed-b144-03b010b6333e",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68378b19-2d0d-4ab0-a7f7-7dee917be844",
   "metadata": {},
   "source": [
    "**Disclamair**: we took and adapted some of the following coding ideas from https://www.geeksforgeeks.org/k-maximum-sums-non-overlapping-contiguous-sub-arrays/ and also from the discussions on\n",
    "https://www.hackerrank.com/challenges/maximum-subarray-sum/problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2f8f0-5808-4196-9848-8a6ba5877d10",
   "metadata": {},
   "source": [
    "Consult for managing back-to-back sequences of requests for appointments. A sequence of requests is of the form `[30, 40, 25, 50, 30, 20]` where each number is the time that the person who makes the appointment wants to spend. Aaccept some requests with a break between them. Two consecutive requests are not accepptable. \n",
    "\n",
    "For example, `[30, 50, 20]` is an acceptable solution (of duration 100), but `[30, 40, 50, 20]` is not, because 30 and 40 are two consecutive appointments. \n",
    "\n",
    "**Goal**: provide a schedule that maximizes the total length of the accepted appointments. Provide also:\n",
    "- an algorithm that computes the acceptable solution with the longest possible duration;\n",
    "- a program that given in input an instance in the form given above, gives the optimal solution\n",
    "\n",
    "For example, in the previous instance, the optimal solution is `[40, 50, 20]`, of total duration 110."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91e0c2-642b-42fa-9dc4-3f0c43acd028",
   "metadata": {},
   "source": [
    "## Formalization of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09862700-a7a9-43f3-b206-b00f59c5c524",
   "metadata": {},
   "source": [
    "Given an array of positive integers, find the maximum sum of all the subsequences with the constraint that no two numbers in the subsequences are adjacent in the array and return both the maximum sum and the subsequence(s) that realize the maximum sum. If $f=f(v)$ is the function we want to implement and $v=(30, 40, 25, 50, 30, 20)$, then we should have $f(v)=(40, 50, 20)$ with sum $s=110$, as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c73156-eb8f-4ef7-8c61-7f72b0a4e28d",
   "metadata": {},
   "source": [
    "**Algorithmic idea: Dynamic programming**. Given an array $v$, let $v^*[i]$ be the optimal solution using the elements with indices $0,..,i$. In order to have a recursive algorithm that terminates set $v^*[0] = v[0]$, and $\\max(v[0],v[1])=0$, then $v^*[i] = \\max(v^*[i - 1], v^*[i - 2] + v[i])$ for $i = 1, ..., n$ (where $n$ is the dimension of the array given in input). Clearly $v^*[n]$ is the solution we want and it is obteined in $O(n)$. We can then use another array to store which choice is made for each subproblem, and so recover the actual elements chosen.\n",
    "\n",
    "The same idea can be used to solve a more general problem as shown in the examples at the end of this paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e353ff2-2b84-4418-a237-dcc2d93ce6bc",
   "metadata": {},
   "source": [
    "*Example* . Let $v=(1,2,2,10,1)$ and consider the matrix \\begin{pmatrix} 1 & 0+2=2 & \\dots & 12 & 4 \\\\ 0 & \\max(0,1)=1 & \\dots &3 & 12  \\end{pmatrix}\n",
    "\n",
    "then the maximum subsequence with no adjecent elements sum is 12 and the elements that realize it are (2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0611da-01c8-4d17-9604-5ade5cf51032",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "53e004f8-19a4-4cd5-b0d6-50649d1917ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows to initialize dictionaries with a lambda function \n",
    "# and provides the default value for a nonexistent key.\n",
    "# so a defaultdict will never raise a KeyError.\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "a5e3489e-0a3f-4b1c-adab-65f3891897d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(array):\n",
    "    # to track sums\n",
    "    sums = [0]*len(array)\n",
    "    \n",
    "    # to track elements of the input array\n",
    "    # example: if array = [1,2,3,5,4] at the emd of the following for loop\n",
    "    # elements = {(0, 1): 1, (0, 2): 2, (1, 3): 4, (2, 5): 7, (4, 4): 8}\n",
    "    elements = defaultdict(lambda: -1)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        # calculate maximum sum \n",
    "        sums[i] = max(sums[i-1], sums[i-2] + array[i])\n",
    "        # memorize\n",
    "        if max(sums[i-1], sums[i-2] + array[i])- (sums[i-2] + array[i]) == 0:\n",
    "            elements[sums[i-2], array[i]] = sums[i]\n",
    "    \n",
    "    # retrieve elements that produce the optimal solution\n",
    "    optimal_subarray = []\n",
    "    \n",
    "    # initialization\n",
    "    max_value = max(elements.values())\n",
    "    count = list(elements.keys())[list(elements.values()).index(max_value)][0]\n",
    "    \n",
    "    \n",
    "    # to print the optimal subarray\n",
    "    # example: if elements = {(15, 11): 26} it means that 15 is the cumulative sum\n",
    "    # in this case 15 = 2+5+4+4 and (2,5,4) is the optimal solution, and 11 is the optimal subsequence sum\n",
    "    # the values stored in the second index are those we need, and the first index we use it to check\n",
    "    # when there are no more elements (i.e. count = 0)\n",
    "    while count != 0:\n",
    "        optimal_value = list(elements.keys())[list(elements.values()).index(max_value)][1]\n",
    "        cum_sum = list(elements.keys())[list(elements.values()).index(max_value)][0]\n",
    "        # put an element that realizes the optimal solution to the list\n",
    "        optimal_subarray.insert(0,optimal_value)\n",
    "\n",
    "        max_value = cum_sum\n",
    "        count = cum_sum\n",
    "\n",
    "    \n",
    "    return optimal_subarray, sums[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172cc0cb-87c6-4673-b98b-d0702c9f0369",
   "metadata": {},
   "source": [
    "### Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "36d0db9f-33a7-4e2d-9f51-e7644a1ac9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 10], 12)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([1,2,2,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4a093364-3c95-45d1-857e-b2b5b6108261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 5, 4], 11)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([1,2,3,5,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "2e2fa4f9-e511-4f29-83ee-645a45a18ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([40, 50, 20], 110)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([30, 40, 25, 50, 30, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99acb0-c37e-41de-8e6f-710223f51486",
   "metadata": {},
   "source": [
    "### Solution of a generalization of the previous problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dac600-279a-4bb7-a3a4-a08b8b9549b7",
   "metadata": {},
   "source": [
    "**Attention:** the following code needs refinement. For example it works poorly in some test cases (e.g. when in the array there are duplicate elements or a lot of contiguous elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "656f2c7e-ea74-4b20-9bde-a4e8fcde04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = defaultdict(lambda: -1)\n",
    "prefix_sum = []\n",
    "trace = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "039ef14e-6e6d-4a33-ac9e-8ce4d3527ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_array_sum(i, j):\n",
    "    \"\"\"\n",
    "    Input: indexes i,j of an array v with i<j\n",
    "    Output: v[i]+v[i+1]+...+v[j-1]+v[j]\n",
    "    Remark: if i>j returns 0\n",
    "    \"\"\"\n",
    "    if i == 0:\n",
    "        return prefix_sum[j]\n",
    "    return (prefix_sum[j] - prefix_sum[i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "089e42a0-9ff2-4530-a7bc-e259e2dffb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_sum(cur, v, k):\n",
    "    \"\"\"\n",
    "    Input: current element cur, array v, positive integer k \n",
    "    Output: current maximum sum \n",
    "    Remark: this function allows also track the elements that realise the maximum sum.      \n",
    "    \"\"\"\n",
    "    if cur >= len(v):\n",
    "        return 0\n",
    "    if dd[cur] != -1:\n",
    "        return dd[cur]\n",
    "    \n",
    "    # use the following line when all the elements in the array are positive, \n",
    "    # else set s1 and s2 to -Infinity\n",
    "    s1 = -1; s2 = -1\n",
    "    \n",
    "    # choose subarray starting at the current element \"cur\"\n",
    "    if cur + k - 1 < len(v):\n",
    "        # Remark: sub_array_sum(cur,cur)=0\n",
    "        s1 = sub_array_sum(cur, cur + k - 1) + maximum_sum(cur + k + 1, v, k)\n",
    "    \n",
    "    # ignore subarray starting at \"cur\"\n",
    "    s2 = maximum_sum(cur + 1, v, k)\n",
    "    dd[cur] = max(s1, s2)\n",
    "    \n",
    "    if s1 >= s2:\n",
    "        # keep track of the elements that realise the maximum sum\n",
    "        trace[cur] = (True, cur + k + 1)\n",
    "        return s1\n",
    "    trace[cur] = (False, cur + 1)\n",
    "    \n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "18956ff6-389e-4db0-8200-cca63d31834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_array(v, trace, k):\n",
    "    \"\"\"\n",
    "    Input: array v, array trace, positive integer k \n",
    "    Output: optimal solution, i.e. optimal subarray\n",
    "    Remark: this function allows to return non-consecutive subarrays of size k \n",
    "            for every positive integer k, but in our problem only the case \n",
    "            k=1 is of interest.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    subArrays = []\n",
    "    for i in range(len(trace)):\n",
    "        if trace[i][0]:\n",
    "            subArrays.append(v[i : i + k])\n",
    "        i = trace[i][1]\n",
    "\n",
    "    return subArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6ae3b76f-bb46-488e-bbae-21a8c7c98bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_solution(v, k):\n",
    "    \"\"\"\n",
    "    Input: array v, positive integer k \n",
    "    Output: optimal solution, i.e. optimal subarray(s)\n",
    "    Remark: this function allows to return non-consecutive optimal subarray(s) of size k \n",
    "            for every positive integer k, but in our problem only the case \n",
    "            k=1 is of interest.\n",
    "    \"\"\"\n",
    "    global dd, trace, prefix_sum\n",
    "    dd = defaultdict(lambda: -1)\n",
    "    \n",
    "    # initialization\n",
    "    trace = [(False, 0)] * len(v)\n",
    "    prefix_sum = [0] * len(v)\n",
    "    prefix_sum[0] = v[0]\n",
    "    \n",
    "    for i in range(1,len(v)):\n",
    "        prefix_sum[i] += prefix_sum[i - 1] + v[i]\n",
    "        \n",
    "    print(\"Array :\", v)\n",
    "    print(\"Max sum: \", maximum_sum(0, v, k))\n",
    "    print(\"Subarrays: \", sub_array(v, trace, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc75da7-c6e4-442e-9ec3-343ee6531a35",
   "metadata": {},
   "source": [
    "### Some examples of solution of a more general problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868418fe-c016-4f49-94bf-46e6501f9d3b",
   "metadata": {},
   "source": [
    "To sole a generalized version of the problem take $k>1$, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "c4259624-7c5b-4e90-9461-d8af72c10bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  9\n",
      "Subarrays:  [[1], [3], [5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7459399c-8d33-4ff5-bf35-606d740d881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  12\n",
      "Subarrays:  [[1, 2], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "43a66d4b-add7-4b3e-90de-908a9e66fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array : [1, 2, 3, 4, 5]\n",
      "Max sum:  12\n",
      "Subarrays:  [[3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "generalized_solution([1,2,3,4,5], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97c792-d536-4cf9-95d3-9a3d7386e7ac",
   "metadata": {},
   "source": [
    "### Alternative solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151e576-06d2-43f0-8440-9cdc78c8a8e0",
   "metadata": {},
   "source": [
    "With immense surprise we have found that it is possible to solve the problem with just 3 lines of code! See https://codegolf.stackexchange.com/questions/183390/maximum-summed-subsequences-with-non-adjacent-items?answertab=active#tab-top for more deatils. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ec227-9771-4476-b4d0-025b553700dc",
   "metadata": {},
   "source": [
    "Here it is the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "7be42ee8-2aa8-49f4-9dd8-33dabe8ca8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [30, 40, 25, 50, 30, 20]\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "b1fcbdd9-97e8-4c8d-9279-def0e250ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 50, 20] 110\n"
     ]
    }
   ],
   "source": [
    "f=lambda a:a and max([a[:1],a[:1]+f(a[2:]),f(a[1:])],key=sum)or a\n",
    "for a, s in [(v, k)]:\n",
    "    print(f(a), sum(f(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "9182b4d2-d128-4b3b-b7ea-a9e34d7745bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 2, 3, 5, 4]\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a28638fd-cb25-4381-96b6-057d8c5973db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4] 8\n"
     ]
    }
   ],
   "source": [
    "f=lambda a:a and max([a[:1],a[:1]+f(a[2:]),f(a[1:])],key=sum)or a\n",
    "for a, s in [(v, k)]:\n",
    "    print(f(a), sum(f(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1aaf79-4701-43dc-ba4b-c578427e62f4",
   "metadata": {},
   "source": [
    "**Credits**: Chas Brown https://codegolf.stackexchange.com/users/69880/chas-brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa62ad-43a1-4afe-9c85-f39e2014b1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
